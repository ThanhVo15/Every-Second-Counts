{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9823351b",
   "metadata": {},
   "source": [
    "# 19/10/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274a9733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **TÃªn bÃ i:** PhÃ¢n loáº¡i Äá»™ dÃ i Chuá»—i Dá»¯ liá»‡u\n",
    "\n",
    "# ### ðŸŽ¯ Má»¥c tiÃªu\n",
    "\n",
    "# Trong phÃ¢n tÃ­ch vÄƒn báº£n (NLP/AI), viá»‡c biáº¿t Ä‘á»™ dÃ i cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o lÃ  ráº¥t quan trá»ng. \n",
    "# HÃ£y viáº¿t má»™t hÃ m Python nháº­n vÃ o má»™t danh sÃ¡ch cÃ¡c chuá»—i (giáº£ Ä‘á»‹nh lÃ  cÃ¡c Ä‘oáº¡n vÄƒn báº£n) vÃ  tráº£ vá» má»™t **tá»« Ä‘iá»ƒn** (dictionary) \n",
    "# tÃ³m táº¯t **sá»‘ láº§n xuáº¥t hiá»‡n** cá»§a cÃ¡c Ä‘á»™ dÃ i chuá»—i Ä‘Ã³.\n",
    "\n",
    "# ### ðŸ“œ YÃªu cáº§u\n",
    "\n",
    "# 1.  Viáº¿t hÃ m `count_string_lengths(data: list[str]) -> dict[int, int]`.\n",
    "# 2.  HÃ m nháº­n vÃ o má»™t danh sÃ¡ch cÃ¡c chuá»—i (`data`).\n",
    "# 3.  Äá»‘i vá»›i má»—i chuá»—i, tÃ­nh Ä‘á»™ dÃ i cá»§a nÃ³.\n",
    "# 4.  Äáº¿m sá»‘ láº§n má»—i Ä‘á»™ dÃ i chuá»—i xuáº¥t hiá»‡n trong danh sÃ¡ch.\n",
    "# 5.  Tráº£ vá» má»™t tá»« Ä‘iá»ƒn trong Ä‘Ã³:\n",
    "#       * **KhÃ³a** lÃ  Ä‘á»™ dÃ i chuá»—i (sá»‘ nguyÃªn).\n",
    "#       * **GiÃ¡ trá»‹** lÃ  sá»‘ láº§n Ä‘á»™ dÃ i Ä‘Ã³ xuáº¥t hiá»‡n (sá»‘ nguyÃªn).\n",
    "\n",
    "# ### ðŸ“ VÃ­ dá»¥\n",
    "\n",
    "# | Input (`data`) | Output (Tá»« Ä‘iá»ƒn) | Giáº£i thÃ­ch |\n",
    "# | :--- | :--- | :--- |\n",
    "# | `[\"AI\", \"Data\", \"Science\", \"ML\", \"Model\"]` | `{2: 2, 4: 1, 7: 1, 5: 1}` | \"AI\", \"ML\" cÃ³ Ä‘á»™ dÃ i 2 (2 láº§n); \"Data\" Ä‘á»™ dÃ i 4 (1 láº§n); \"Science\" Ä‘á»™ dÃ i 7 (1 láº§n); \"Model\" Ä‘á»™ dÃ i 5 (1 láº§n). |\n",
    "# | `[\"\", \"A\", \"BB\", \"A\"]` | `{0: 1, 1: 2, 2: 1}` | Chuá»—i rá»—ng Ä‘á»™ dÃ i 0 (1 láº§n); \"A\" Ä‘á»™ dÃ i 1 (2 láº§n); \"BB\" Ä‘á»™ dÃ i 2 (1 láº§n). |\n",
    "\n",
    "# # Kiá»ƒm tra\n",
    "# test_data_1 = [\"AI\", \"Data\", \"Science\", \"ML\", \"Model\"]\n",
    "# print(f\"Input: {test_data_1}\")\n",
    "# print(f\"Output: {count_string_lengths(test_data_1)}\")\n",
    "\n",
    "# test_data_2 = [\"\", \"A\", \"BB\", \"A\"]\n",
    "# print(f\"Input: {test_data_2}\")\n",
    "# print(f\"Output: {count_string_lengths(test_data_2)}\")\n",
    "# ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b245514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['AI', 'Data', 'Science', 'ML', 'Model']\n",
      "Output: {2: 2, 4: 1, 7: 1, 5: 1}\n",
      "Input: ['', 'A', 'BB', 'A']\n",
      "Output: {0: 1, 1: 2, 2: 1}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def count_string_lengths(data: List[str]) -> List[str]:\n",
    "    result = defaultdict(int)\n",
    "\n",
    "    for word in data:\n",
    "        length = len(word)\n",
    "\n",
    "        result[length] +=1\n",
    "    return dict(result)\n",
    "        \n",
    "\n",
    "# # Kiá»ƒm tra\n",
    "test_data_1 = [\"AI\", \"Data\", \"Science\", \"ML\", \"Model\"]\n",
    "print(f\"Input: {test_data_1}\")\n",
    "print(f\"Output: {count_string_lengths(test_data_1)}\")\n",
    "\n",
    "test_data_2 = [\"\", \"A\", \"BB\", \"A\"]\n",
    "print(f\"Input: {test_data_2}\")\n",
    "print(f\"Output: {count_string_lengths(test_data_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d864511b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'A': 1, 'I': 1}),\n",
       " Counter({'a': 2, 'D': 1, 't': 1}),\n",
       " Counter({'c': 2, 'e': 2, 'S': 1, 'i': 1, 'n': 1}),\n",
       " Counter({'M': 1, 'L': 1}),\n",
       " Counter({'M': 1, 'o': 1, 'd': 1, 'e': 1, 'l': 1})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_1 = [\"AI\", \"Data\", \"Science\", \"ML\", \"Model\"]\n",
    "result = []\n",
    "for word in test_data_1:\n",
    "    c = Counter(word)\n",
    "    result.append(c)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab05947",
   "metadata": {},
   "source": [
    "# 20/10/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca52d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **Táº­p trung vÃ o:** PhÃ¢n tÃ­ch dá»¯ liá»‡u, Xá»­ lÃ½ chuá»—i, TÆ° duy logic cho viá»‡c tiá»n xá»­ lÃ½ dá»¯ liá»‡u (Pre-processing) trong AI/ML.\n",
    "\n",
    "# ### TÃªn bÃ i toÃ¡n: PhÃ¢n TÃ­ch Cáº£m XÃºc CÆ¡ Báº£n (Basic Sentiment Analysis)\n",
    "\n",
    "# ### MÃ´ táº£\n",
    "\n",
    "# Trong lÄ©nh vá»±c Xá»­ lÃ½ NgÃ´n ngá»¯ Tá»± nhiÃªn (NLP), viá»‡c trÃ­ch xuáº¥t cáº£m xÃºc tá»« vÄƒn báº£n lÃ  bÆ°á»›c quan trá»ng. \n",
    "# Báº¡n Ä‘Æ°á»£c cung cáº¥p má»™t **danh sÃ¡ch cÃ¡c cÃ¢u** (vÃ­ dá»¥: bÃ¬nh luáº­n, Ä‘Ã¡nh giÃ¡). \n",
    "# Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  viáº¿t má»™t hÃ m Python Ä‘á»ƒ phÃ¢n loáº¡i má»—i cÃ¢u thÃ nh \n",
    "# **TÃ­ch Cá»±c** (Positive), **TiÃªu Cá»±c** (Negative) hoáº·c **Trung TÃ­nh** (Neutral) dá»±a trÃªn sá»± xuáº¥t hiá»‡n cá»§a cÃ¡c tá»« khÃ³a.\n",
    "\n",
    "# ### YÃªu cáº§u\n",
    "\n",
    "# Viáº¿t má»™t hÃ m Python `analyze_sentiment(sentences)` nháº­n vÃ o má»™t danh sÃ¡ch cÃ¡c chuá»—i (cÃ¢u) vÃ \n",
    "#  tráº£ vá» má»™t danh sÃ¡ch cÃ¡c chuá»—i káº¿t quáº£ ('TÃ­ch Cá»±c', 'TiÃªu Cá»±c', hoáº·c 'Trung TÃ­nh').\n",
    "\n",
    "# **CÃ¡c quy táº¯c phÃ¢n loáº¡i:**\n",
    "\n",
    "# 1.  **TÃ­ch Cá»±c** náº¿u cÃ¢u chá»©a Ã­t nháº¥t má»™t trong cÃ¡c tá»« khÃ³a: **'great', 'excellent', 'love', 'amazing'**.\n",
    "# 2.  **TiÃªu Cá»±c** náº¿u cÃ¢u chá»©a Ã­t nháº¥t má»™t trong cÃ¡c tá»« khÃ³a: **'bad', 'worst', 'poor', 'disappointed'**.\n",
    "# 3.  **Trung TÃ­nh** náº¿u cÃ¢u khÃ´ng chá»©a tá»« khÃ³a TÃ­ch Cá»±c hay TiÃªu Cá»±c nÃ o.\n",
    "# 4.  ***LÆ°u Ã½:*** **Æ¯u tiÃªn TÃ­ch Cá»±c** hÆ¡n TiÃªu Cá»±c náº¿u má»™t cÃ¢u chá»©a cáº£ tá»« khÃ³a TÃ­ch Cá»±c vÃ  TiÃªu Cá»±c.\n",
    "# 5.  Viá»‡c kiá»ƒm tra **khÃ´ng phÃ¢n biá»‡t chá»¯ hoa, chá»¯ thÆ°á»ng** (case-insensitive).\n",
    "\n",
    "# ### VÃ­ dá»¥\n",
    "\n",
    "# | Input (sentences) | Output (sentiment results) | Giáº£i thÃ­ch |\n",
    "# | :--- | :--- | :--- |\n",
    "# | `[\"This is a great product.\"]` | `['TÃ­ch Cá»±c']` | Chá»©a tá»« 'great'. |\n",
    "# | `[\"The service was bad.\"]` | `['TiÃªu Cá»±c']` | Chá»©a tá»« 'bad'. |\n",
    "# | `[\"It was okay, not good.\"]` | `['Trung TÃ­nh']` | KhÃ´ng chá»©a tá»« khÃ³a nÃ o. |\n",
    "# | `[\"The quality was great but the price was bad.\"]` | `['TÃ­ch Cá»±c']` | Chá»©a cáº£ 'great' vÃ  'bad', Æ°u tiÃªn TÃ­ch Cá»±c. |\n",
    "\n",
    "# ### Lá»i giáº£i (áº¨n)\n",
    "\n",
    "# ```python\n",
    "# def analyze_sentiment(sentences):\n",
    "#     \"\"\"\n",
    "#     PhÃ¢n loáº¡i cáº£m xÃºc cÆ¡ báº£n cho má»™t danh sÃ¡ch cÃ¡c cÃ¢u.\n",
    "#     Æ¯u tiÃªn TÃ­ch Cá»±c hÆ¡n TiÃªu Cá»±c.\n",
    "#     \"\"\"\n",
    "#     positive_words = {'great', 'excellent', 'love', 'amazing'}\n",
    "#     negative_words = {'bad', 'worst', 'poor', 'disappointed'}\n",
    "#     results = []\n",
    "\n",
    "#     for sentence in sentences:\n",
    "#         # Chuyá»ƒn cÃ¢u thÃ nh chá»¯ thÆ°á»ng Ä‘á»ƒ kiá»ƒm tra khÃ´ng phÃ¢n biá»‡t chá»¯ hoa, chá»¯ thÆ°á»ng\n",
    "#         sentence_lower = sentence.lower()\n",
    "\n",
    "#         # Kiá»ƒm tra sá»± xuáº¥t hiá»‡n cá»§a tá»« khÃ³a\n",
    "#         is_positive = any(word in sentence_lower for word in positive_words)\n",
    "#         is_negative = any(word in sentence_lower for word in negative_words)\n",
    "\n",
    "#         # Ãp dá»¥ng quy táº¯c phÃ¢n loáº¡i (Æ¯u tiÃªn TÃ­ch Cá»±c)\n",
    "#         if is_positive:\n",
    "#             results.append('TÃ­ch Cá»±c')\n",
    "#         elif is_negative:\n",
    "#             results.append('TiÃªu Cá»±c')\n",
    "#         else:\n",
    "#             results.append('Trung TÃ­nh')\n",
    "\n",
    "#     return results\n",
    "\n",
    "# # VÃ­ dá»¥ kiá»ƒm tra:\n",
    "# test_sentences = [\n",
    "#     \"This is a great product.\",\n",
    "#     \"The service was bad.\",\n",
    "#     \"It was okay, not good.\",\n",
    "#     \"The quality was great but the price was bad.\",\n",
    "#     \"Nothing special here.\"\n",
    "# ]\n",
    "\n",
    "# # print(analyze_sentiment(test_sentences))\n",
    "# # Káº¿t quáº£ mong Ä‘á»£i: ['TÃ­ch Cá»±c', 'TiÃªu Cá»±c', 'Trung TÃ­nh', 'TÃ­ch Cá»±c', 'Trung TÃ­nh']\n",
    "# ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0136884b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive', 'Netral', 'Negative', 'Positive', 'Negative']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "def analyze_sentiment(sentences: List[str]) -> List[str]:\n",
    "    positive_words = {'great', 'excellent', 'love', 'amazing'}\n",
    "    negative_words = {'bad', 'worst', 'poor', 'disappointed'}\n",
    "    results = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_lower = sentence.lower()\n",
    "\n",
    "        is_positive = any(word in sentence_lower for word in positive_words)\n",
    "        is_negative = any(word in sentence_lower for word in negative_words)\n",
    "\n",
    "        if is_positive == True:\n",
    "            results.append(\"Positive\")\n",
    "        elif is_negative == False:\n",
    "            results.append(\"Negative\")\n",
    "        else:\n",
    "            results.append(\"Netral\")\n",
    "\n",
    "    return results\n",
    "        \n",
    "\n",
    "\n",
    "test_sentences = [\n",
    "    \"This is a great product.\",\n",
    "    \"The service was bad.\",\n",
    "    \"It was okay, not good.\",\n",
    "    \"The quality was great but the price was bad.\",\n",
    "    \"Nothing special here.\"\n",
    "]\n",
    "\n",
    "print(analyze_sentiment(test_sentences))\n",
    "# Káº¿t quáº£ mong Ä‘á»£i: ['TÃ­ch Cá»±c', 'TiÃªu Cá»±c', 'Trung TÃ­nh', 'TÃ­ch Cá»±c', 'Trung TÃ­nh']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb86f215",
   "metadata": {},
   "source": [
    "# 21/10/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36214367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### ðŸŽ¯ Má»¥c tiÃªu\n",
    "\n",
    "# Luyá»‡n táº­p ká»¹ nÄƒng **xá»­ lÃ½ chuá»—i (string manipulation)** vÃ  **tÃ­nh toÃ¡n táº§n suáº¥t (frequency counting)**, \n",
    "# má»™t ká»¹ nÄƒng cÆ¡ báº£n trong **Tiá»n xá»­ lÃ½ Dá»¯ liá»‡u (Data Preprocessing)** vÃ  **Xá»­ lÃ½ NgÃ´n ngá»¯ Tá»± nhiÃªn (NLP)**.\n",
    "\n",
    "# ### ðŸ“œ YÃªu cáº§u BÃ i toÃ¡n: TÃ¬m Tá»« ÄÆ¡n Äá»™c\n",
    "\n",
    "# Cho má»™t danh sÃ¡ch cÃ¡c cÃ¢u (má»™t máº£ng chuá»—i), hÃ£y tÃ¬m ra **táº¥t cáº£ cÃ¡c tá»«** chá»‰ xuáº¥t hiá»‡n **Ä‘Ãºng má»™t láº§n** trÃªn *toÃ n bá»™* danh sÃ¡ch cÃ¢u Ä‘Ã³. \n",
    "# CÃ¡c tá»« khÃ´ng phÃ¢n biá»‡t chá»¯ hoa/chá»¯ thÆ°á»ng.\n",
    "\n",
    "# **VÃ­ dá»¥:**\n",
    "\n",
    "#   * **Äáº§u vÃ o:** `sentences = [\"The cat sat on the mat.\", \"A dog chased the cat.\", \"The bird flew.\"]`\n",
    "\n",
    "#   * **Äáº§u ra Mong muá»‘n (Danh sÃ¡ch cÃ¡c tá»«):** `[\"sat\", \"on\", \"mat\", \"a\", \"dog\", \"chased\", \"bird\", \"flew\"]`\n",
    "\n",
    "#   * **Giáº£i thÃ­ch:**\n",
    "\n",
    "#       * \"the\" vÃ  \"cat\" xuáº¥t hiá»‡n nhiá»u hÆ¡n má»™t láº§n.\n",
    "#       * \"sat\", \"on\", \"mat\", \"a\", \"dog\", \"chased\", \"bird\", \"flew\" má»—i tá»« chá»‰ xuáº¥t hiá»‡n Ä‘Ãºng má»™t láº§n.\n",
    "\n",
    "# ### ðŸ› ï¸ HÃ m Cáº§n Triá»ƒn khai\n",
    "\n",
    "# ```python\n",
    "# def find_unique_words(sentences: list[str]) -> list[str]:\n",
    "#     \"\"\"\n",
    "#     TÃ¬m táº¥t cáº£ cÃ¡c tá»« chá»‰ xuáº¥t hiá»‡n Ä‘Ãºng má»™t láº§n trong toÃ n bá»™ danh sÃ¡ch cÃ¢u.\n",
    "    \n",
    "#     :param sentences: Danh sÃ¡ch cÃ¡c chuá»—i cÃ¢u.\n",
    "#     :return: Danh sÃ¡ch cÃ¡c tá»« Ä‘Æ¡n Ä‘á»™c.\n",
    "#     \"\"\"\n",
    "#     pass\n",
    "# ```\n",
    "\n",
    "# ### ðŸ’¡ Gá»£i Ã½\n",
    "\n",
    "# 1.  Cáº§n pháº£i chuáº©n hÃ³a táº¥t cáº£ cÃ¡c tá»« vá» **chá»¯ thÆ°á»ng**.\n",
    "# 2.  Loáº¡i bá» cÃ¡c **dáº¥u cÃ¢u** (vÃ­ dá»¥: `.`, `,`).\n",
    "# 3.  Sá»­ dá»¥ng má»™t **tá»« Ä‘iá»ƒn (dictionary)** hoáº·c **`collections.Counter`** Ä‘á»ƒ Ä‘áº¿m táº§n suáº¥t cá»§a má»—i tá»«."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b1653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sat', 'on', 'mat', 'a', 'dog', 'chased', 'bird', 'flew']\n"
     ]
    }
   ],
   "source": [
    "# from typing import List\n",
    "# from collections import Counter\n",
    "# import string\n",
    "\n",
    "# def find_unique_words(sentences: List[str]) -> List[str]:\n",
    "#     all_words = []\n",
    "\n",
    "#     for sentence in sentences:\n",
    "#         sentence_lower = sentence.lower()\n",
    "#         sentence_clean = sentence_lower.translate(str.maketrans('', '', string.punctuation))\n",
    "#         words = sentence_clean.split()\n",
    "#         all_words.extend(words)\n",
    "\n",
    "#     word_counts = Counter(all_words)\n",
    "#     result = [word for word, count in word_counts.items() if count == 1]\n",
    "\n",
    "#     return result\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     sentences = [\"The cat sat on the mat.\", \"A dog chased the cat.\", \"The bird flew.\"]\n",
    "#     result = find_unique_words(sentences)\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e89afd",
   "metadata": {},
   "source": [
    "# 22/10/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371377f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### ðŸ“Š Ngá»¯ cáº£nh & Má»¥c tiÃªu: Chuáº©n hÃ³a Dá»¯ liá»‡u (Normalization)\n",
    "\n",
    "# Trong tiá»n xá»­ lÃ½ dá»¯ liá»‡u cho Machine Learning/AI, **Chuáº©n hÃ³a Min-Max (Min-Max Normalization)** \n",
    "# lÃ  má»™t ká»¹ thuáº­t phá»• biáº¿n Ä‘á»ƒ chuyá»ƒn Ä‘á»•i cÃ¡c Ä‘áº·c trÆ°ng (features) vá» má»™t pháº¡m vi cá»‘ Ä‘á»‹nh, \n",
    "# thÆ°á»ng lÃ  $[0, 1]$. Äiá»u nÃ y giÃºp cÃ¡c thuáº­t toÃ¡n há»c (Ä‘áº·c biá»‡t lÃ  cÃ¡c thuáº­t toÃ¡n dá»±a trÃªn khoáº£ng cÃ¡ch nhÆ° K-Means, KNN) \n",
    "# hoáº¡t Ä‘á»™ng hiá»‡u quáº£ hÆ¡n vÃ  ngÄƒn cháº·n cÃ¡c Ä‘áº·c trÆ°ng cÃ³ pháº¡m vi lá»›n chi phá»‘i.\n",
    "\n",
    "# **YÃªu cáº§u:**\n",
    "\n",
    "# Viáº¿t má»™t hÃ m Python cÃ³ tÃªn `min_max_normalize` nháº­n vÃ o má»™t **danh sÃ¡ch cÃ¡c sá»‘ nguyÃªn hoáº·c sá»‘ thá»±c (list of numbers)** \n",
    "# vÃ  tráº£ vá» má»™t **danh sÃ¡ch má»›i** chá»©a cÃ¡c giÃ¡ trá»‹ Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a theo cÃ´ng thá»©c sau:\n",
    "\n",
    "# $$\n",
    "# X_{\\text{norm}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
    "# $$\n",
    "\n",
    "# Trong Ä‘Ã³:\n",
    "# * $X$ lÃ  giÃ¡ trá»‹ gá»‘c.\n",
    "# * $X_{\\text{min}}$ lÃ  giÃ¡ trá»‹ nhá» nháº¥t trong danh sÃ¡ch.\n",
    "# * $X_{\\text{max}}$ lÃ  giÃ¡ trá»‹ lá»›n nháº¥t trong danh sÃ¡ch.\n",
    "\n",
    "# **LÆ°u Ã½ quan trá»ng:**\n",
    "# * Pháº£i xá»­ lÃ½ trÆ°á»ng há»£p Ä‘áº·c biá»‡t: Náº¿u $X_{\\text{max}} = X_{\\text{min}}$ (táº¥t cáº£ cÃ¡c sá»‘ Ä‘á»u báº±ng nhau), \n",
    "# hÃ m nÃªn tráº£ vá» má»™t danh sÃ¡ch cÃ¡c sá»‘ $0.0$ cÃ³ cÃ¹ng Ä‘á»™ dÃ i.\n",
    "# * Káº¿t quáº£ pháº£i lÃ  danh sÃ¡ch cÃ¡c sá»‘ thá»±c (float).\n",
    "\n",
    "# ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78772dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [10, 20, 30, 40, 50] -> Output: [0.0, 0.25, 0.5, 0.75, 1.0]\n",
      "Input: [5, 5, 5, 5] -> Output: [0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "def min_max_normalize(data: List[Union[int, float]]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Chuáº©n hÃ³a Min-Max má»™t danh sÃ¡ch cÃ¡c sá»‘ vá» khoáº£ng [0, 1].\n",
    "\n",
    "    Args:\n",
    "        data: Danh sÃ¡ch cÃ¡c sá»‘ (int hoáº·c float).\n",
    "\n",
    "    Returns:\n",
    "        Danh sÃ¡ch cÃ¡c sá»‘ Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return []\n",
    "\n",
    "    max_x = max(data)\n",
    "    min_x = min(data)\n",
    "\n",
    "    # Xá»­ lÃ½ trÆ°á»ng há»£p táº¥t cáº£ cÃ¡c pháº§n tá»­ báº±ng nhau\n",
    "    if min_x == max_x:\n",
    "        return [0.0] * len(data)  # Tráº£ vá» list cÃ¡c sá»‘ float 0.0\n",
    "    \n",
    "    # Sá»­ dá»¥ng list comprehension cho ngáº¯n gá»n vÃ  hiá»‡u quáº£\n",
    "    return [(x - min_x) / (max_x - min_x) for x in data]\n",
    "\n",
    "# Kiá»ƒm tra\n",
    "data1 = [10, 20, 30, 40, 50]\n",
    "print(f\"Input: {data1} -> Output: {min_max_normalize(data1)}\")\n",
    "\n",
    "data2 = [5, 5, 5, 5]\n",
    "print(f\"Input: {data2} -> Output: {min_max_normalize(data2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7700cddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.3333333333333333, 0.6666666666666666, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Input 1\n",
    "data1 = [10, 20, 30, 40, 50]\n",
    "# Expected Output: [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "# (Min=10, Max=50 -> Denominator=40. (10-10)/40=0.0, (20-10)/40=0.25, ...)\n",
    "\n",
    "\n",
    "# Input 2\n",
    "data2 = [5, 5, 5, 5]\n",
    "# Expected Output: [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "\n",
    "# Input 3\n",
    "data3 = [-5, 0, 5, 10]\n",
    "# Expected Output: [0.0, 0.3333333333333333, 0.6666666666666666, 1.0]\n",
    "data1_res = min_max_normalize(data3)\n",
    "print(data1_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dc23a5",
   "metadata": {},
   "source": [
    "# D11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d28c3b",
   "metadata": {},
   "source": [
    "# NgÃ y 11 - Daily Learning Challenge\n",
    "\n",
    "ðŸ§® **PART 1: PYTHON BASIC PRACTICE**\n",
    "\n",
    "1. **TÃªn bÃ i toÃ¡n & mÃ´ táº£ ngáº¯n gá»n thá»±c táº¿**: \"Tá»‘i Æ°u hÃ³a dá»¯ liá»‡u lá»›n vá»›i vectorization\" - Xá»­ lÃ½ táº­p dá»¯ liá»‡u lá»›n vá» doanh sá»‘ bÃ¡n hÃ ng Ä‘á»ƒ tÃ­nh toÃ¡n metrics kinh doanh (nhÆ° tá»•ng doanh thu, trung bÃ¬nh theo category) má»™t cÃ¡ch hiá»‡u quáº£, sá»­ dá»¥ng vectorization Ä‘á»ƒ trÃ¡nh loop cháº­m, há»— trá»£ ETL scale lá»›n.\n",
    "\n",
    "2. **VÃ­ dá»¥ input/output máº«u**:\n",
    "   - Input: DataFrame máº«u (pandas):\n",
    "     ```python\n",
    "     import pandas as pd\n",
    "     import numpy as np\n",
    "     data = pd.DataFrame({\n",
    "         'ProductID': [1, 2, 3, 4, 5, 6],\n",
    "         'Category': ['A', 'A', 'B', 'B', 'A', 'C'],\n",
    "         'Revenue': [100, 200, 150, 300, 120, 250],\n",
    "         'Cost': [50, 100, 70, 150, 60, 120]\n",
    "     })\n",
    "     ```\n",
    "   - Output: DataFrame vá»›i metrics: tá»•ng revenue theo category, profit (revenue - cost), vÃ­ dá»¥: {'A': {'TotalRevenue': 420, 'AvgProfit': 70}}.\n",
    "\n",
    "3. **Má»¥c tiÃªu há»c táº­p (lÃ½ thuyáº¿t chÃ­nh)**: Hiá»ƒu vectorization trong NumPy/Pandas Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u nhanh, trÃ¡nh for-loops, Ã¡p dá»¥ng broadcasting vÃ  groupby vectorized cho performance cao á»Ÿ level nÃ¢ng cao.\n",
    "\n",
    "4. **Gá»£i Ã½ tÆ° duy giáº£i**: PhÃ¢n tÃ­ch: Convert df sang NumPy arrays cho calc nhanh. Sá»­ dá»¥ng groupby vá»›i agg cho metrics. Ãp dá»¥ng vector ops nhÆ° subtraction cho profit. Xá»­ lÃ½ NaN báº±ng fillna, Ä‘áº£m báº£o dtype phÃ¹ há»£p cho speed.\n",
    "\n",
    "5. **Code tá»‘i Æ°u**:\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   import numpy as np\n",
    "\n",
    "   # BÆ°á»›c 1: Load dá»¯ liá»‡u\n",
    "   data = pd.DataFrame({  # Dá»¯ liá»‡u máº«u lá»›n hÆ¡n\n",
    "       'ProductID': np.arange(1, 11),\n",
    "       'Category': ['A', 'A', 'B', 'B', 'A', 'C', 'C', 'B', 'A', 'C'],\n",
    "       'Revenue': [100, 200, 150, 300, 120, 250, 180, 220, 140, 190],\n",
    "       'Cost': [50, 100, 70, 150, 60, 120, 90, 110, 70, 95]\n",
    "   })\n",
    "\n",
    "   # BÆ°á»›c 2: Vectorized calculations\n",
    "   data['Profit'] = data['Revenue'] - data['Cost']  # Vector subtraction\n",
    "\n",
    "   # BÆ°á»›c 3: Groupby vectorized\n",
    "   grouped = data.groupby('Category').agg(\n",
    "       TotalRevenue=('Revenue', 'sum'),\n",
    "       AvgProfit=('Profit', 'mean')\n",
    "   ).reset_index()\n",
    "\n",
    "   # BÆ°á»›c 4: Convert to dict cho output\n",
    "   metrics_dict = grouped.set_index('Category').to_dict(orient='index')\n",
    "\n",
    "   print(metrics_dict)\n",
    "   ```\n",
    "   **Giáº£i thÃ­ch chi tiáº¿t**:\n",
    "   - Load: Táº¡o df vá»›i NumPy cho ID nhanh.\n",
    "   - Calc: Vector op '-' trÃªn columns, khÃ´ng loop.\n",
    "   - Groupby: agg vá»›i functions built-in, vectorized internal.\n",
    "   - Output: set_index vÃ  to_dict cho cáº¥u trÃºc dá»… dÃ¹ng, best practice Pandas.\n",
    "\n",
    "6. **Tá»‘i Æ°u (complexity, code style)**: Time complexity: O(n) nhá» vectorization (so vá»›i O(n) loop nhÆ°ng nhanh hÆ¡n constant factor). Style: PEP8, ngáº¯n gá»n, trÃ¡nh unnecessary vars. Modular: CÃ³ thá»ƒ wrap vÃ o function def compute_metrics(df).\n",
    "\n",
    "7. **LiÃªn há»‡ thá»±c táº¿**: Sá»­ dá»¥ng trong ETL script Ä‘á»ƒ process log bÃ¡n hÃ ng lá»›n, dáº«n dáº¯t vÃ o advanced báº±ng cÃ¡ch tÃ­ch há»£p vá»›i ML Ä‘á»ƒ dá»± bÃ¡o dá»±a trÃªn metrics nÃ y, vÃ­ dá»¥ feed vÃ o model optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77873b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': {'TotalRevenue': 420, 'AvgProfit': 70.0},\n",
       " 'B': {'TotalRevenue': 450, 'AvgProfit': 115.0},\n",
       " 'C': {'TotalRevenue': 250, 'AvgProfit': 130.0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.DataFrame({\n",
    "    'ProductID': [1, 2, 3, 4, 5, 6],\n",
    "    'Category': ['A', 'A', 'B', 'B', 'A', 'C'],\n",
    "    'Revenue': [100, 200, 150, 300, 120, 250],\n",
    "    'Cost': [50, 100, 70, 150, 60, 120]\n",
    "})\n",
    "\n",
    "df['Profit'] = df['Revenue'] - df['Cost']\n",
    "df_group = df.groupby('Category').agg(\n",
    "    TotalRevenue = ('Revenue', 'sum'),\n",
    "    AvgProfit = ('Profit', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "df_group = df_group.set_index('Category').to_dict(orient = 'index')\n",
    "df_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7604741",
   "metadata": {},
   "source": [
    "ðŸ§® **PART 2: PYTHON ADVANCED PRACTICE**\n",
    "\n",
    "1. **TÃªn bÃ i toÃ¡n & mÃ´ táº£ ngáº¯n gá»n thá»±c táº¿**: \"Tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh ML vá»›i hyperparameter tuning vÃ  cloud integration\" - XÃ¢y dá»±ng pipeline ML Ä‘á»ƒ dá»± bÃ¡o doanh thu tá»« dá»¯ liá»‡u lá»‹ch sá»­, sá»­ dá»¥ng GridSearchCV cho tuning, tÃ­ch há»£p Airflow cho scheduling vÃ  cloud storage (giáº£ láº­p S3) Ä‘á»ƒ scale real-world.\n",
    "\n",
    "2. **VÃ­ dá»¥ input/output**:\n",
    "   - Input: DataFrame máº«u (pandas):\n",
    "     ```python\n",
    "     import pandas as pd\n",
    "     data = pd.DataFrame({\n",
    "         'Month': [1, 2, 3, 4, 5, 6],\n",
    "         'AdsSpend': [1000, 1500, 1200, 1800, 1400, 2000],\n",
    "         'PreviousRevenue': [5000, 6000, 5500, 7000, 6500, 8000],\n",
    "         'Revenue': [5200, 6300, 5700, 7200, 6700, 8200]\n",
    "     })\n",
    "     ```\n",
    "   - Output: Best params tá»« tuning, vÃ­ dá»¥: {'n_estimators': 100, 'max_depth': 5}, vÃ  RMSE evaluate.\n",
    "\n",
    "3. **Má»¥c tiÃªu há»c táº­p (thuáº­t toÃ¡n/lÃ½ thuyáº¿t chÃ­nh)**: NÃ¢ng cao vá»›i hyperparameter optimization (GridSearchCV), modular code, tÃ­ch há»£p DAG Airflow cho production, evaluate vá»›i cross-validation scale.\n",
    "\n",
    "4. **Gá»£i Ã½ tÆ° duy giáº£i**: PhÃ¢n tÃ­ch: Feature engineering (lag features tá»« previous). Split data, define param grid cho RandomForest. Train vá»›i GridSearch, vectorized fit. Giáº£ láº­p cloud save model. Evaluate RMSE, handle overfit báº±ng CV.\n",
    "\n",
    "5. **Code tá»‘i Æ°u**:\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "   from sklearn.ensemble import RandomForestRegressor\n",
    "   from sklearn.metrics import mean_squared_error\n",
    "   import joblib\n",
    "   import os  # Giáº£ láº­p cloud save\n",
    "\n",
    "   # BÆ°á»›c 1: Load vÃ  feature engineering\n",
    "   data = pd.DataFrame({  # Dá»¯ liá»‡u máº«u\n",
    "       'Month': np.arange(1, 11),\n",
    "       'AdsSpend': [1000, 1500, 1200, 1800, 1400, 2000, 1600, 2200, 1300, 1900],\n",
    "       'PreviousRevenue': [5000, 6000, 5500, 7000, 6500, 8000, 7500, 8500, 7000, 9000],\n",
    "       'Revenue': [5200, 6300, 5700, 7200, 6700, 8200, 7700, 8700, 7200, 9200]\n",
    "   })\n",
    "   X = data[['AdsSpend', 'PreviousRevenue']]\n",
    "   y = data['Revenue']\n",
    "\n",
    "   # BÆ°á»›c 2: Split data\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "   # BÆ°á»›c 3: Hyperparameter tuning\n",
    "   model = RandomForestRegressor(random_state=42)\n",
    "   param_grid = {'n_estimators': [50, 100], 'max_depth': [3, 5, None]}\n",
    "   grid_search = GridSearchCV(model, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "   grid_search.fit(X_train, y_train)  # Vectorized internal\n",
    "\n",
    "   # BÆ°á»›c 4: Evaluate\n",
    "   best_model = grid_search.best_estimator_\n",
    "   predictions = best_model.predict(X_test)\n",
    "   rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "   print(f\"Best Params: {grid_search.best_params_}, RMSE: {rmse}\")\n",
    "\n",
    "   # BÆ°á»›c 5: Save model (giáº£ láº­p cloud)\n",
    "   joblib.dump(best_model, 'model.pkl')\n",
    "   # Trong Airflow: def train_and_save(): ... ; dag = DAG('ml_pipeline', schedule_interval='@monthly'); task = PythonOperator(task_id='train', python_callable=train_and_save, dag=dag)\n",
    "   ```\n",
    "   **Giáº£i thÃ­ch chi tiáº¿t**:\n",
    "   - Load/engineering: Chá»n features, no loop.\n",
    "   - Split: train_test_split vectorized.\n",
    "   - Tuning: GridSearchCV tá»± Ä‘á»™ng CV, fit on df.\n",
    "   - Evaluate: predict vector, MSE.\n",
    "   - Save: joblib cho persistence, comment Airflow integration cho production.\n",
    "\n",
    "6. **Tá»‘i Æ°u (complexity, code style, modularization)**: Time complexity: O(grid_size * cv * n * log n) cho RF - scale vá»›i dask náº¿u lá»›n. Refactor: TÃ¡ch def tune_model(X, y). Style: PEP8, modular funcs. TÃ­ch há»£p cloud: Sá»­ dá»¥ng boto3 cho S3 save real, Airflow cho orchestrate.\n",
    "\n",
    "7. **LiÃªn há»‡ thá»±c táº¿**: Pipeline nÃ y dÃ¹ng trong BI Ä‘á»ƒ forecast revenue, schedule Airflow trÃªn cloud (AWS EMR) cho big data, deploy model serving vá»›i FastAPI, há»— trá»£ decision-making scale.\n",
    "\n",
    "ðŸ’¡ **FINAL SECTION: DAILY REFLECTION**\n",
    "\n",
    "Gá»£i Ã½ báº¡n ghi láº¡i 3 Ä‘iá»u sau trong nháº­t kÃ½ há»c táº­p:  \n",
    "1. **Há»c Ä‘Æ°á»£c gÃ¬ hÃ´m nay?** (VÃ­ dá»¥: Vectorization nÃ¢ng cao trong Pandas cho data lá»›n, vÃ  hyperparameter tuning vá»›i GridSearch trong ML pipeline).  \n",
    "2. **Váº¥n Ä‘á» mÃ¬nh gáº·p lÃ  gÃ¬?** (VÃ­ dá»¥: Groupby cháº­m - debug báº±ng profile; hoáº·c overfit - tÄƒng CV folds).  \n",
    "3. **Náº¿u lÃ m láº¡i, mÃ¬nh sáº½ tá»‘i Æ°u Ä‘iá»u gÃ¬?** (VÃ­ dá»¥: ThÃªm Numba cho vector ops custom, hoáº·c dÃ¹ng Ray cho distributed tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9083d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (2.3.1)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/60.8 kB ? eta -:--:--\n",
      "     ------------------- ------------------ 30.7/60.8 kB 325.1 kB/s eta 0:00:01\n",
      "     ------------------- ------------------ 30.7/60.8 kB 325.1 kB/s eta 0:00:01\n",
      "     ------------------------------- ------ 51.2/60.8 kB 217.9 kB/s eta 0:00:01\n",
      "     -------------------------------------- 60.8/60.8 kB 269.0 kB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.2-cp311-cp311-win_amd64.whl (8.9 MB)\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.9 MB 1.3 MB/s eta 0:00:07\n",
      "   ---------------------------------------- 0.1/8.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ---------------------------------------- 0.1/8.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ---------------------------------------- 0.1/8.9 MB 1.1 MB/s eta 0:00:09\n",
      "    --------------------------------------- 0.2/8.9 MB 807.1 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.2/8.9 MB 901.1 kB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.3/8.9 MB 1.1 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.3/8.9 MB 1.1 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.4/8.9 MB 1.1 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.5/8.9 MB 1.2 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.7/8.9 MB 1.5 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.8/8.9 MB 1.5 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.0/8.9 MB 1.7 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.2/8.9 MB 1.9 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.6/8.9 MB 2.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.8/8.9 MB 2.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.0/8.9 MB 2.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.4/8.9 MB 3.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.0/8.9 MB 3.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.5/8.9 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.1/8.9 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.6/8.9 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.9/8.9 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.0/8.9 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.0/8.9 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.0/8.9 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.0/8.9 MB 4.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.2/8.9 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.3/8.9 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.3/8.9 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.3/8.9 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.3/8.9 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.4/8.9 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.6/8.9 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.7/8.9 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.7/8.9 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.7/8.9 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.7/8.9 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.7/8.9 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.7/8.9 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.7/8.9 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.7/8.9 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.7/8.9 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.8/8.9 MB 3.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.9/8.9 MB 3.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.1/8.9 MB 3.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.1/8.9 MB 3.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.1/8.9 MB 3.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.2/8.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.3/8.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.3/8.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.3/8.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.3/8.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.3/8.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.3/8.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.3/8.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.3/8.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.3/8.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.3/8.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.3/8.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.3/8.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.4/8.9 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.4/8.9 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.9/8.9 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.9/8.9 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.9/8.9 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.9/8.9 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.1/8.9 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.1/8.9 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.1/8.9 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.1/8.9 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.1/8.9 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.1/8.9 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.1/8.9 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.1/8.9 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.1/8.9 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.1/8.9 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.4/8.9 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.5/8.9 MB 2.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.5/8.9 MB 2.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.5/8.9 MB 2.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.5/8.9 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.6/8.9 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.6/8.9 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.6/8.9 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.6/8.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.8/8.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.8/8.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.9/8.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.0/8.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.0/8.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.1/8.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.2/8.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.3/8.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.3/8.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.4/8.9 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.5/8.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.7/8.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.7/8.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.9/8.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.9/8.9 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "   ---------------------------------------- 0.0/308.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 308.4/308.4 kB 9.6 MB/s eta 0:00:00\n",
      "Downloading scipy-1.16.3-cp311-cp311-win_amd64.whl (38.7 MB)\n",
      "   ---------------------------------------- 0.0/38.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.7/38.7 MB 14.6 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 1.3/38.7 MB 13.3 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 1.4/38.7 MB 12.9 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 1.4/38.7 MB 12.9 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 1.4/38.7 MB 12.9 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 1.4/38.7 MB 12.9 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 1.5/38.7 MB 4.5 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 2.1/38.7 MB 6.1 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 2.1/38.7 MB 6.1 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 2.1/38.7 MB 6.1 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 2.4/38.7 MB 4.6 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 2.8/38.7 MB 5.1 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 2.8/38.7 MB 5.1 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 2.8/38.7 MB 5.1 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 3.1/38.7 MB 4.5 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 3.5/38.7 MB 4.8 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 3.5/38.7 MB 4.8 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 3.5/38.7 MB 4.8 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 3.9/38.7 MB 4.4 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 4.2/38.7 MB 4.6 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 4.2/38.7 MB 4.6 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 4.2/38.7 MB 4.1 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 4.7/38.7 MB 4.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 4.8/38.7 MB 4.5 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 4.8/38.7 MB 4.5 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 4.9/38.7 MB 4.1 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 5.1/38.7 MB 4.1 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 5.3/38.7 MB 4.1 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 5.3/38.7 MB 4.1 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 5.4/38.7 MB 3.9 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 5.9/38.7 MB 4.1 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 6.5/38.7 MB 4.3 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 7.0/38.7 MB 4.6 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 7.6/38.7 MB 4.8 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 8.2/38.7 MB 5.0 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 8.8/38.7 MB 5.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 9.4/38.7 MB 5.5 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 10.0/38.7 MB 5.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 10.5/38.7 MB 5.7 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 11.1/38.7 MB 5.7 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 11.5/38.7 MB 5.7 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 12.1/38.7 MB 6.4 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 12.6/38.7 MB 7.0 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 13.2/38.7 MB 7.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 13.8/38.7 MB 8.3 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 14.3/38.7 MB 8.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 14.9/38.7 MB 9.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 15.5/38.7 MB 10.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 16.0/38.7 MB 12.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 16.6/38.7 MB 12.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 17.2/38.7 MB 12.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 17.7/38.7 MB 12.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 18.3/38.7 MB 12.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 18.8/38.7 MB 12.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 19.4/38.7 MB 12.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 20.0/38.7 MB 12.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 20.6/38.7 MB 12.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 21.2/38.7 MB 12.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 21.7/38.7 MB 12.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 22.3/38.7 MB 12.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 22.8/38.7 MB 12.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 23.3/38.7 MB 12.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 23.8/38.7 MB 12.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 24.4/38.7 MB 12.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 25.0/38.7 MB 12.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 25.6/38.7 MB 12.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 26.1/38.7 MB 12.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 26.7/38.7 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 27.2/38.7 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 27.8/38.7 MB 12.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 28.4/38.7 MB 12.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 28.9/38.7 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 29.5/38.7 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 30.1/38.7 MB 12.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 30.6/38.7 MB 12.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.2/38.7 MB 12.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.8/38.7 MB 12.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 32.3/38.7 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 32.9/38.7 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.3/38.7 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.8/38.7 MB 12.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.4/38.7 MB 12.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 34.9/38.7 MB 12.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.5/38.7 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.1/38.7 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.6/38.7 MB 12.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.2/38.7 MB 12.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.7/38.7 MB 12.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  37.8/38.7 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  37.8/38.7 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  37.8/38.7 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  37.9/38.7 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.7/38.7 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.7/38.7 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.7/38.7 MB 9.6 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.7.2 scipy-1.16.3 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikit-learn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f601bb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 3, 'n_estimators': 25}, RMSE: 259250.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Import Data\n",
    "df = pd.DataFrame({\n",
    "    'Month': np.arange(1, 11),\n",
    "    'AdsSpend': [1000, 1500, 1200, 1800, 1400, 2000, 1600, 2200, 1300, 1900],\n",
    "    'PreviousRevenue': [5000, 6000, 5500, 7000, 6500, 8000, 7500, 8500, 7000, 9000],\n",
    "    'Revenue': [5200, 6300, 5700, 7200, 6700, 8200, 7700, 8700, 7200, 9200]\n",
    "})\n",
    "\n",
    "X = df[['AdsSpend', 'PreviousRevenue']]\n",
    "y = df[['Revenue']]\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 42)\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "param_grid = {'n_estimators': [25,50,100], 'max_depth': [3,5,7,None]}\n",
    "grid_search_cv = GridSearchCV(model, param_grid=param_grid, cv = 3, scoring = 'neg_mean_squared_error')\n",
    "grid_search_cv.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Evaluate\n",
    "best_model = grid_search_cv.best_estimator_\n",
    "predictions = best_model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Best params: {grid_search_cv.best_params_}, RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba52d6b",
   "metadata": {},
   "source": [
    "\n",
    "# NgÃ y 13 - Daily Learning Challenge\n",
    "\n",
    "ðŸ§® **PART 1: PYTHON BASIC PRACTICE**\n",
    "\n",
    "1. **TÃªn bÃ i toÃ¡n & mÃ´ táº£ ngáº¯n gá»n thá»±c táº¿**: \"Xá»­ lÃ½ dá»¯ liá»‡u lá»›n vá»›i parallel apply\" - Xá»­ lÃ½ táº­p dá»¯ liá»‡u lá»›n vá» giao dá»‹ch khÃ¡ch hÃ ng Ä‘á»ƒ Ã¡p dá»¥ng transformation phá»©c táº¡p (nhÆ° tÃ­nh risk score dá»±a trÃªn pattern), sá»­ dá»¥ng parallel processing trong pandas Ä‘á»ƒ tÄƒng tá»‘c, há»— trá»£ ETL scale mÃ  khÃ´ng cáº§n framework lá»›n.\n",
    "\n",
    "2. **VÃ­ dá»¥ input/output máº«u**:\n",
    "   - Input: DataFrame máº«u (pandas):\n",
    "     ```python\n",
    "     import pandas as pd\n",
    "     data = pd.DataFrame({\n",
    "         'TransactionID': [1, 2, 3, 4, 5],\n",
    "         'Amount': [100, 2000, 150, 300, 12000],\n",
    "         'Frequency': [5, 1, 3, 2, 1],\n",
    "         'CustomerID': [101, 102, 101, 103, 104]\n",
    "     })\n",
    "     ```\n",
    "   - Output: DataFrame vá»›i cá»™t má»›i 'RiskScore' (vÃ­ dá»¥: Amount > 1000 and Frequency < 2 -> High, else Low), nhÆ°: ['Low', 'High', 'Low', 'Low', 'High'].\n",
    "\n",
    "3. **Má»¥c tiÃªu há»c táº­p (lÃ½ thuyáº¿t chÃ­nh)**: Hiá»ƒu parallel apply trong Pandas vá»›i pandarallel hoáº·c joblib Ä‘á»ƒ xá»­ lÃ½ transformation cháº­m trÃªn large data, tÄƒng dáº§n tá»« vectorization cÆ¡ báº£n, táº­p trung best practices cho intermediate.\n",
    "\n",
    "4. **Gá»£i Ã½ tÆ° duy giáº£i**: PhÃ¢n tÃ­ch: Define function cho row-wise calc (khÃ´ng vectorizable dá»…). Init pandarallel, apply parallel. Xá»­ lÃ½ NaN báº±ng fill, Ä‘áº£m báº£o function pure cho parallel safe.\n",
    "\n",
    "5. **Code tá»‘i Æ°u**:\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   from pandarallel import pandarallel\n",
    "\n",
    "   # BÆ°á»›c 1: Init parallel (cÃ i náº¿u cáº§n, nhÆ°ng giáº£ láº­p Ä‘Ã£ cÃ³)\n",
    "   pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "   # BÆ°á»›c 2: Load dá»¯ liá»‡u\n",
    "   data = pd.DataFrame({  # Dá»¯ liá»‡u máº«u lá»›n hÆ¡n\n",
    "       'TransactionID': range(1, 11),\n",
    "       'Amount': [100, 2000, 150, 300, 12000, 500, 2500, 400, 18000, 600],\n",
    "       'Frequency': [5, 1, 3, 2, 1, 4, 1, 3, 1, 2],\n",
    "       'CustomerID': [101, 102, 101, 103, 104, 101, 105, 103, 106, 104]\n",
    "   })\n",
    "\n",
    "   # BÆ°á»›c 3: Define function cho apply\n",
    "   def calculate_risk(row):\n",
    "       if row['Amount'] > 1000 and row['Frequency'] < 2:\n",
    "           return 'High'\n",
    "       else:\n",
    "           return 'Low'\n",
    "\n",
    "   # BÆ°á»›c 4: Parallel apply\n",
    "   data['RiskScore'] = data.parallel_apply(calculate_risk, axis=1)\n",
    "\n",
    "   print(data[['TransactionID', 'RiskScore']])\n",
    "   ```\n",
    "   **Giáº£i thÃ­ch chi tiáº¿t**:\n",
    "   - Init: pandarallel.initialize() cho multi-core.\n",
    "   - Load: Táº¡o df vá»›i data Ä‘a dáº¡ng.\n",
    "   - Function: Simple logic if-else, row-based.\n",
    "   - Apply: parallel_apply thay apply thÃ´ng thÆ°á»ng, axis=1 cho row.\n",
    "\n",
    "6. **Tá»‘i Æ°u (complexity, code style)**: Time complexity: O(n / cores) nhá» parallel (tá»« O(n) sequential). Style: PEP8, function riÃªng cho modular. Náº¿u large data, check cores available; fallback vectorized náº¿u cÃ³ thá»ƒ (np.where).\n",
    "\n",
    "7. **LiÃªn há»‡ thá»±c táº¿**: DÃ¹ng trong ETL script Ä‘á»ƒ score risk trÃªn transaction logs lá»›n, dáº«n dáº¯t advanced báº±ng tÃ­ch há»£p vá»›i ML model phá»©c táº¡p hÆ¡n trong parallel, vÃ­ dá»¥ feed score vÃ o clustering.\n",
    "\n",
    "ðŸ§® **PART 2: PYTHON ADVANCED PRACTICE**\n",
    "\n",
    "1. **TÃªn bÃ i toÃ¡n & mÃ´ táº£ ngáº¯n gá»n thá»±c táº¿**: \"XÃ¢y dá»±ng pipeline AI vá»›i distributed training\" - Xá»­ lÃ½ dá»¯ liá»‡u lá»›n Ä‘á»ƒ train neural network dá»± bÃ¡o inventory sá»­ dá»¥ng PyTorch Distributed, tÃ­ch há»£p cloud (giáº£ láº­p Dask hoáº·c torch.distributed), evaluate táº¡i scale real-world.\n",
    "\n",
    "2. **VÃ­ dá»¥ input/output**:\n",
    "   - Input: DataFrame máº«u (pandas):\n",
    "     ```python\n",
    "     import pandas as pd\n",
    "     data = pd.DataFrame({\n",
    "         'TimeStep': [1, 2, 3, 4, 5, 6],\n",
    "         'Demand': [100, 120, 110, 130, 115, 140],\n",
    "         'PreviousInventory': [500, 480, 460, 450, 430, 415],\n",
    "         'Inventory': [480, 460, 450, 430, 415, 400]\n",
    "     })\n",
    "     ```\n",
    "   - Output: Trained model loss, predictions sample, vÃ­ dá»¥: [118, 128] vá»›i low loss.\n",
    "\n",
    "3. **Má»¥c tiÃªu há»c táº­p (thuáº­t toÃ¡n/lÃ½ thuyáº¿t chÃ­nh)**: NÃ¢ng cao vá»›i distributed training PyTorch (torch.distributed), DataParallel cho multi-GPU/cloud, modular pipeline vá»›i Airflow scheduling.\n",
    "\n",
    "4. **Gá»£i Ã½ tÆ° duy giáº£i**: PhÃ¢n tÃ­ch: Feature tá»« lag/demand. Init distributed, define NN model. Train vá»›i DDP (DistributedDataParallel). Evaluate distributed, handle sync. Giáº£ láº­p multi-process.\n",
    "\n",
    "5. **Code tá»‘i Æ°u**:\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   import torch\n",
    "   import torch.nn as nn\n",
    "   import torch.distributed as dist\n",
    "   from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "   from torch.utils.data import DataLoader, TensorDataset\n",
    "   from sklearn.model_selection import train_test_split\n",
    "\n",
    "   # BÆ°á»›c 1: Init distributed (giáº£ láº­p local multi-process)\n",
    "   dist.init_process_group(backend='gloo', init_method='tcp://localhost:23456', world_size=1, rank=0)\n",
    "\n",
    "   # BÆ°á»›c 2: Load vÃ  prep data\n",
    "   data = pd.DataFrame({  # Dá»¯ liá»‡u máº«u\n",
    "       'TimeStep': range(1, 11),\n",
    "       'Demand': [100, 120, 110, 130, 115, 140, 125, 135, 118, 145],\n",
    "       'PreviousInventory': [500, 480, 460, 450, 430, 415, 400, 380, 365, 350],\n",
    "       'Inventory': [480, 460, 450, 430, 415, 400, 380, 365, 350, 330]\n",
    "   })\n",
    "   X = torch.tensor(data[['Demand', 'PreviousInventory']].values, dtype=torch.float32)\n",
    "   y = torch.tensor(data['Inventory'].values, dtype=torch.float32)\n",
    "   train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2)\n",
    "   train_dataset = TensorDataset(train_X, train_y)\n",
    "   train_loader = DataLoader(train_dataset, batch_size=4)\n",
    "\n",
    "   # BÆ°á»›c 3: Define model\n",
    "   class InventoryNet(nn.Module):\n",
    "       def __init__(self):\n",
    "           super().__init__()\n",
    "           self.fc1 = nn.Linear(2, 16)\n",
    "           self.fc2 = nn.Linear(16, 1)\n",
    "\n",
    "       def forward(self, x):\n",
    "           x = torch.relu(self.fc1(x))\n",
    "           return self.fc2(x)\n",
    "\n",
    "   model = InventoryNet()\n",
    "   model = DDP(model)  # Distributed\n",
    "\n",
    "   # BÆ°á»›c 4: Train\n",
    "   criterion = nn.MSELoss()\n",
    "   optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "   for epoch in range(50):\n",
    "       for inputs, labels in train_loader:\n",
    "           optimizer.zero_grad()\n",
    "           outputs = model(inputs)\n",
    "           loss = criterion(outputs.squeeze(), labels)\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "\n",
    "   # BÆ°á»›c 5: Evaluate\n",
    "   with torch.no_grad():\n",
    "       predictions = model(test_X).squeeze().numpy()\n",
    "       rmse = torch.sqrt(criterion(model(test_X).squeeze(), test_y)).item()\n",
    "   print(f\"RMSE: {rmse}, Sample Predictions: {predictions[:2]}\")\n",
    "\n",
    "   dist.destroy_process_group()\n",
    "   # Trong Airflow: def distributed_train(): ... ; dag = DAG('ai_pipeline', schedule='@weekly'); task = PythonOperator(...)\n",
    "   ```\n",
    "   **Giáº£i thÃ­ch chi tiáº¿t**:\n",
    "   - Init: dist.init_process_group cho distributed.\n",
    "   - Prep: Tensor tá»« df, DataLoader.\n",
    "   - Model: Simple NN, wrap DDP.\n",
    "   - Train: Standard loop, backward sync auto.\n",
    "   - Evaluate: No grad, RMSE metric.\n",
    "\n",
    "6. **Tá»‘i Æ°u (complexity, code style, modularization)**: Time complexity: O(epochs * n / nodes) distributed. Refactor: TÃ¡ch def init_distributed(). Style: PEP8, class model. Cloud: Sá»­ dá»¥ng torch.distributed trÃªn AWS EC2 multi-node; modular vá»›i hydra config.\n",
    "\n",
    "7. **LiÃªn há»‡ thá»±c táº¿**: Pipeline cho forecast inventory lá»›n, schedule Airflow trÃªn Kubernetes, deploy model TorchServe cloud, há»— trá»£ supply chain scale.\n",
    "\n",
    "ðŸ’¡ **FINAL SECTION: DAILY REFLECTION**\n",
    "\n",
    "Gá»£i Ã½ báº¡n ghi láº¡i 3 Ä‘iá»u sau trong nháº­t kÃ½ há»c táº­p:  \n",
    "1. **Há»c Ä‘Æ°á»£c gÃ¬ hÃ´m nay?** (VÃ­ dá»¥: Parallel apply trong Pandas cho transformation lá»›n, vÃ  distributed training PyTorch cho AI scale).  \n",
    "2. **Váº¥n Ä‘á» mÃ¬nh gáº·p lÃ  gÃ¬?** (VÃ­ dá»¥: Parallel init fail - debug env; hoáº·c sync issues - check backend).  \n",
    "3. **Náº¿u lÃ m láº¡i, mÃ¬nh sáº½ tá»‘i Æ°u Ä‘iá»u gÃ¬?** (VÃ­ dá»¥: ThÃªm swifter cho fallback parallel, hoáº·c Horovod cho better distributed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a34b608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f601e10dd7174a37a369eb6d7be943f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1), Label(value='0 / 1'))), HBox(câ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TransactionID RiskScore\n",
      "0              1       Low\n",
      "1              2      High\n",
      "2              3       Low\n",
      "3              4       Low\n",
      "4              5      High\n"
     ]
    }
   ],
   "source": [
    "# Part 1: \n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'TransactionID': [1, 2, 3, 4, 5],\n",
    "    'Amount': [100, 2000, 150, 300, 12000],\n",
    "    'Frequency': [5, 1, 3, 2, 1],\n",
    "    'CustomerID': [101, 102, 101, 103, 104]\n",
    "})\n",
    "\n",
    "def calculate_risk(row):\n",
    "    if row['Amount'] > 1000 and row['Frequency'] < 2:\n",
    "        return 'High'\n",
    "    else:\n",
    "        return 'Low'\n",
    "    \n",
    "data['RiskScore'] = data.parallel_apply(calculate_risk, axis=1)\n",
    "\n",
    "print(data[['TransactionID', 'RiskScore']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ebcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c774d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Init distributed (giáº£ láº­p local multi-process)\n",
    "dist.init_process_group(\n",
    "    backend='gloo',\n",
    "    init_method='tcp://localhost:23456',\n",
    "    world_size=1,\n",
    "    rank=0\n",
    ")\n",
    "# Step 2: Load vÃ  prep data\n",
    "data = pd.DataFrame({  # Dá»¯ liá»‡u máº«u\n",
    "    'TimeStep': range(1, 11),\n",
    "    'Demand': [100, 120, 110, 130, 115, 140, 125, 135, 118, 145],\n",
    "    'PreviousInventory': [500, 480, 460, 450, 430, 415, 400, 380, 365, 350],\n",
    "    'Inventory': [480, 460, 450, 430, 415, 400, 380, 365, 350, 330]\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
